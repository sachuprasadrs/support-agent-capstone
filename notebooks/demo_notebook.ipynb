{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c88fd029",
   "metadata": {},
   "source": [
    "# Context-Aware Customer Support Agent ‚Äî Demo\n",
    "**Kaggle Capstone ‚Äî Stylish Demo Notebook**\n",
    "\n",
    "This notebook demonstrates:\n",
    "- Live multi-turn demos using the local agent\n",
    "- Memory inspection (short & long term)\n",
    "- Trace-based observability (traces.jl)\n",
    "- Hybrid evaluation (heuristic + LLM-as-judge)\n",
    "- Visualizations and summary metrics for judges\n",
    "\n",
    "> Run cells top ‚Üí bottom. If imports fail, ensure you opened the project root in VS Code / Jupyter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac271f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe7b1287",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'agent'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Project imports (must run notebook from project root)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01magent\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01morchestrator\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m handle_user_message\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01magent\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmemory\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m retrieve_memories, save_user, init_db\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01magent\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlogger\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m log_event\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'agent'"
     ]
    }
   ],
   "source": [
    "# Setup & imports\n",
    "import os, json, time\n",
    "from IPython.display import Markdown, display, HTML\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Project imports (must run notebook from project root)\n",
    "from agent.orchestrator import handle_user_message\n",
    "from agent.memory import retrieve_memories, save_user, init_db\n",
    "from agent.logger import log_event\n",
    "from agent.evaluator import hybrid_score\n",
    "\n",
    "# Notebook styling helpers\n",
    "plt.style.use(\"seaborn-whitegrid\")\n",
    "%matplotlib inline\n",
    "\n",
    "display(Markdown(\"**Environment check:**\"))\n",
    "print(\"Python:\", os.sys.version.splitlines()[0])\n",
    "print(\"Working dir:\", os.getcwd())\n",
    "display(Markdown(\"---\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d815c54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity checks\n",
    "init_db()  # safe to call repeatedly\n",
    "\n",
    "if not os.path.exists(\"traces.jl\"):\n",
    "    open(\"traces.jl\",\"a\").close()\n",
    "\n",
    "if not os.path.exists(\"evaluation_report.json\"):\n",
    "    # If you haven't run evaluate_batch, create a lightweight placeholder so notebook cells work\n",
    "    placeholder = []\n",
    "    with open(\"evaluation_report.json\",\"w\") as f:\n",
    "        json.dump(placeholder, f)\n",
    "    print(\"Created placeholder evaluation_report.json ‚Äî run evaluate_batch for real results.\")\n",
    "\n",
    "display(Markdown(\"‚úÖ Sanity checks complete.\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trace loader + pretty function\n",
    "def load_traces(limit=None):\n",
    "    traces = []\n",
    "    if os.path.exists(\"traces.jl\"):\n",
    "        with open(\"traces.jl\",\"r\") as f:\n",
    "            for i,line in enumerate(f):\n",
    "                if not line.strip(): \n",
    "                    continue\n",
    "                try:\n",
    "                    traces.append(json.loads(line))\n",
    "                except:\n",
    "                    continue\n",
    "                if limit and len(traces) >= limit:\n",
    "                    break\n",
    "    return traces\n",
    "\n",
    "def show_trace(trace_id):\n",
    "    traces = load_traces()\n",
    "    seq = [t for t in traces if t.get(\"trace_id\") == trace_id]\n",
    "    if not seq:\n",
    "        display(Markdown(f\"**No trace found for** `{trace_id}`\"))\n",
    "        return\n",
    "    display(Markdown(f\"### Trace `{trace_id}` ‚Äî {len(seq)} events\"))\n",
    "    for ev in seq:\n",
    "        ts = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime(ev.get(\"timestamp\", 0)))\n",
    "        kind = ev.get(\"event\",\"?\")\n",
    "        content = {k:v for k,v in ev.items() if k not in [\"timestamp\",\"trace_id\",\"event\"]}\n",
    "        display(Markdown(f\"- **{ts}** `{kind}` ‚Äî `{json.dumps(content)}`\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befdc7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single-turn demonstration\n",
    "display(Markdown(\"## üß™ Single-turn demo\"))\n",
    "\n",
    "session_id = \"demo_session_1\"\n",
    "user_id = \"u001\"\n",
    "# Ensure user exists (will create or overwrite)\n",
    "save_user(user_id, \"Demo User\", \"demo@example.com\", {\"notes\":\"notebook demo\"})\n",
    "\n",
    "inp = \"My order A123 is late ‚Äî where is it?\"\n",
    "display(Markdown(f\"**User:** {inp}\"))\n",
    "\n",
    "out = handle_user_message(session_id=session_id, user_id=user_id, user_msg=inp)\n",
    "display(Markdown(f\"**Agent reply:** {out['reply']}\"))\n",
    "display(Markdown(f\"**Trace id:** `{out['trace_id']}`\"))\n",
    "\n",
    "# show the trace for this run (most recent trace)\n",
    "show_trace(out[\"trace_id\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d4dea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multi-turn conversation sample\n",
    "display(Markdown(\"## üó£Ô∏è Multi-turn conversation demo\"))\n",
    "\n",
    "session_id = \"demo_session_2\"\n",
    "user_id = \"u002\"\n",
    "save_user(user_id, \"Multi Demo\", \"multidemo@example.com\", {})\n",
    "\n",
    "turns = [\n",
    "    \"What is the status of my order A123?\",\n",
    "    \"It says delivered but I never received it.\",\n",
    "    \"Please help ‚Äî I want a refund if it can't be found.\"\n",
    "]\n",
    "\n",
    "for i, msg in enumerate(turns, 1):\n",
    "    display(Markdown(f\"**Turn {i} ‚Äî User:** {msg}\"))\n",
    "    res = handle_user_message(session_id=session_id, user_id=user_id, user_msg=msg)\n",
    "    display(Markdown(f\"- **Agent:** {res['reply']}  \\n- **Trace:** `{res['trace_id']}`\"))\n",
    "    show_trace(res['trace_id'])\n",
    "    display(Markdown(\"---\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fd03a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect memories for a user\n",
    "display(Markdown(\"## üß† Memory inspection\"))\n",
    "\n",
    "target_user = \"u002\"\n",
    "m = retrieve_memories(target_user, limit=10)\n",
    "display(Markdown(f\"Memories for **{target_user}** ‚Äî showing up to 10\"))\n",
    "for i, mem in enumerate(m,1):\n",
    "    display(Markdown(f\"- [{i}] **{mem['mem_type']}** ‚Äî {mem['content']}\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca5d8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load evaluation_report.json (created by evaluate_batch)\n",
    "display(Markdown(\"## üìà Evaluation report summary\"))\n",
    "\n",
    "if os.path.exists(\"evaluation_report.json\"):\n",
    "    with open(\"evaluation_report.json\",\"r\") as f:\n",
    "        eval_data = json.load(f)\n",
    "else:\n",
    "    eval_data = []\n",
    "\n",
    "# If eval_data is empty, create a small synthetic sample so plots render\n",
    "if not eval_data:\n",
    "    eval_data = [\n",
    "        {\"case\": \"Where is my order A123?\", \"reply\": \"I created ticket T1\", \"scores\": {\"heuristic\":0.6,\"llm_resolution\":0.5,\"llm_helpfulness\":0.7,\"final_score\":0.6}},\n",
    "        {\"case\": \"I want a refund\", \"reply\": \"Refund started\", \"scores\": {\"heuristic\":0.8,\"llm_resolution\":0.7,\"llm_helpfulness\":0.7,\"final_score\":0.73}}\n",
    "    ]\n",
    "\n",
    "# Normalize to DataFrame\n",
    "rows = []\n",
    "for r in eval_data:\n",
    "    sc = r.get(\"scores\",{})\n",
    "    rows.append({\n",
    "        \"case\": r.get(\"case\",\"\"),\n",
    "        \"reply\": r.get(\"reply\",\"\"),\n",
    "        \"heuristic\": sc.get(\"heuristic\", np.nan),\n",
    "        \"llm_resolution\": sc.get(\"llm_resolution\", np.nan),\n",
    "        \"llm_helpfulness\": sc.get(\"llm_helpfulness\", np.nan),\n",
    "        \"final_score\": sc.get(\"final_score\", np.nan)\n",
    "    })\n",
    "df_eval = pd.DataFrame(rows)\n",
    "display(df_eval.head())\n",
    "display(Markdown(f\"**Avg final score:** {df_eval['final_score'].mean():.3f}\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32affb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final score distribution\n",
    "display(Markdown(\"## üìä Final score distribution\"))\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.hist(df_eval[\"final_score\"], bins=10, edgecolor=\"k\", alpha=0.8)\n",
    "plt.title(\"Final Score Distribution\")\n",
    "plt.xlabel(\"Final score\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.axvline(df_eval[\"final_score\"].mean(), color=\"red\", linestyle=\"--\", label=f\"Mean: {df_eval['final_score'].mean():.2f}\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f04a4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Component comparison scatter\n",
    "display(Markdown(\"## ‚öñÔ∏è Heuristic vs LLM (resolution)\"))\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.scatter(df_eval[\"heuristic\"], df_eval[\"llm_resolution\"], s=60, alpha=0.8)\n",
    "plt.xlabel(\"Heuristic score\")\n",
    "plt.ylabel(\"LLM resolution score\")\n",
    "plt.title(\"Heuristic vs LLM resolution\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb7b4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count tool usage in traces\n",
    "display(Markdown(\"## üîß Tool usage frequency (from traces)\"))\n",
    "\n",
    "traces = load_traces()\n",
    "tool_calls = []\n",
    "for ev in traces:\n",
    "    if ev.get(\"event\") == \"tool_result\" or ev.get(\"event\")==\"tool\":\n",
    "        tname = ev.get(\"tool\") or (ev.get(\"event\") if ev.get(\"tool\") is None else ev.get(\"tool\"))\n",
    "        tool_calls.append(tname)\n",
    "tool_counts = pd.Series(tool_calls).value_counts()\n",
    "if tool_counts.empty:\n",
    "    display(Markdown(\"_No tool calls recorded yet._\"))\n",
    "else:\n",
    "    display(tool_counts.to_frame(\"count\"))\n",
    "    tool_counts.plot(kind=\"barh\", color=\"tab:orange\")\n",
    "    plt.title(\"Tool usage frequency\")\n",
    "    plt.xlabel(\"Count\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16250020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory impact (simple demonstration)\n",
    "display(Markdown(\"## üßæ Memory impact (demo)\"))\n",
    "\n",
    "# We do a small AB test simulation: for same prompt, ask agent with/without memory context\n",
    "test_user = \"u050\"\n",
    "prompt = \"Where is my order A123?\"\n",
    "\n",
    "# Save a memory for user (simulate prior conversation)\n",
    "add_mem = {\"mem_type\":\"note\",\"content\":\"User prefers fast delivery, previously complained about late shipments\"}\n",
    "# Ensure db has this\n",
    "from agent.memory import add_memory as _add_memory\n",
    "_add_memory(test_user, add_mem[\"mem_type\"], add_mem[\"content\"])\n",
    "\n",
    "# With memory\n",
    "res_with = handle_user_message(session_id=\"ab_with\", user_id=test_user, user_msg=prompt)\n",
    "\n",
    "# Without memory: remove memory temporarily or use new user\n",
    "res_without = handle_user_message(session_id=\"ab_without\", user_id=\"ab_new_user\", user_msg=prompt)\n",
    "\n",
    "display(Markdown(f\"- **With memory reply:** {res_with['reply']}\"))\n",
    "display(Markdown(f\"- **Without memory reply:** {res_without['reply']}\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e00d65",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Architecture summary (short)\n",
    "\n",
    "- **Orchestrator** ‚Äî think/act/observe loop; calls tools and records traces.  \n",
    "- **Tools** ‚Äî small deterministic functions (order lookup, ticket creation, product lookup).  \n",
    "- **Memory** ‚Äî SQLite-backed long-term memory + session events.  \n",
    "- **Evaluator** ‚Äî hybrid (heuristic + LLM-as-judge) and batch runner.  \n",
    "- **Observability** ‚Äî `traces.jl` with event-level logs for every decision.\n",
    "\n",
    "**What judges want to see**\n",
    "- Clear problem statement and demo  \n",
    "- Traces showing chain-of-thought (tool calls + results)  \n",
    "- Memory usage examples (personalization)  \n",
    "- Evaluation metrics and visualization  \n",
    "- A short demo video + README\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f537e58",
   "metadata": {},
   "source": [
    "### Notebook complete\n",
    "\n",
    "- Save this notebook.\n",
    "- Run all cells to produce visual outputs.\n",
    "- Export as HTML (File ‚Üí Export) and include the HTML + notebook in your Kaggle submission.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
